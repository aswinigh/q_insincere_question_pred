{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_data = pd.read_csv(\"../input/train.csv\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "6569748209a144ca0ee5952498737c990fe8d283"
      },
      "cell_type": "code",
      "source": "train_data.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cf99ac3caf857c6f7bdc57403c2f7309fb187854"
      },
      "cell_type": "code",
      "source": "import string",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "629f2a042a235aed15cb6c189a1f4d5fa0396fb5"
      },
      "cell_type": "code",
      "source": "question_text = train_data['question_text'].str.replace('[^\\w\\s]','')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "09812e4bb57021678cd96bf25788271d5f57c7be"
      },
      "cell_type": "code",
      "source": "question_text = question_text.str.split(' ')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6a1d21d8e00eef7112f42f6958dda91aac2642bf"
      },
      "cell_type": "code",
      "source": "question_text.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "43ad93fd4d585c6fb2f76da3fbb24f9799ac93cb"
      },
      "cell_type": "code",
      "source": "q_list = []\nfor i,x in enumerate(question_text[:]):\n   if isinstance(x,list): \n      if len(x)!=0:\n        for y in x:\n            q_list.append(y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c19b55ae8b411bbcaa44c41e1133de7ff5edd89a"
      },
      "cell_type": "code",
      "source": "q_list",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d32d77ea5d968ab6f0dc4104c6187b14bbc7a450"
      },
      "cell_type": "code",
      "source": "from collections import Counter\ncounts = Counter(q_list)\nq_vocab = sorted(counts, key=counts.get, reverse=True)\nq_vocab_to_int = {word: ii for ii, word in enumerate(q_vocab, 1)}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5ebe40815531d46b854b91898eb7e483e14a4c81"
      },
      "cell_type": "code",
      "source": "q_int=[]\nfor q in question_text:\n  q_int.append([q_vocab_to_int[word] for word in q])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b77840fb5d7deb1ac86ccdb4166a2f0e75c9fc81"
      },
      "cell_type": "code",
      "source": "#q_int",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6fdd38e2d7cad90e05aaf704cc002095f2e5e11f"
      },
      "cell_type": "code",
      "source": "def pad_features(title_ints, seq_length):\n    ''' Return features of review_ints, where each review is padded with 0's \n        or truncated to the input seq_length.\n    '''\n    \n    # getting the correct rows x cols shape\n    features = np.zeros((len(title_ints), seq_length), dtype=int)\n\n    # for each review, I grab that review and \n    for i, row in enumerate(title_ints):\n        features[i, -len(row):] = np.array(row)[:seq_length]\n    \n    return features",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "76e66827083b0c4a0c593ee05404b64234431e65"
      },
      "cell_type": "code",
      "source": "seq_length = 25\n\nq_features = pad_features(q_int, seq_length=seq_length)\n\n## test statements - do not change - ##\nassert len(q_features)==len(q_int), \"Your features should have as many rows as reviews.\"\nassert len(q_features[0])==seq_length, \"Each feature row should contain seq_length values.\"\nprint(q_features[:30,:25])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "82cb1d0eea4e27d1c8baadf63d8ececa01603945"
      },
      "cell_type": "code",
      "source": "q_labels = train_data['target']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b29009ba32ecf121d47a0de7e7a1370b858f6dbb"
      },
      "cell_type": "code",
      "source": "#q_labels = q_labels.tolist()\nq_labels = np.array(q_labels)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "b704d356cc129d7b548ddfb1831750a1230f2be5"
      },
      "cell_type": "code",
      "source": "split_frac = 0.6\n\n## split data into training, validation, and test data (features and labels, x and y)\nbatch_size = 20\nsplit_idx = int(len(q_features)*0.9)\nsplit_idx = split_idx - (split_idx%batch_size)\ntrain_x, val_x = q_features[:split_idx], q_features[split_idx+2:]\ntrain_y, val_y = q_labels[:split_idx], q_labels[split_idx+2:]\n\n#test_idx = int(len(remaining_x)*0.2)\n#test_idx = test_idx - (test_idx%batch_size)\n#val_x, test_x = remaining_x[:test_idx], remaining_x[:-test_idx+1]\n#val_y, test_y = remaining_y[:test_idx], remaining_y[:-test_idx+1]\n\n## print out the shapes of your resultant feature data\nprint(\"\\t\\t\\tFeature Shapes:\")\nprint(\"Train set: \\t\\t{}\".format(train_x.shape), \n      \"\\nTrain_y: \\t\\t{}\".format(train_y.shape),\n      \"\\nValidation set: \\t{}\".format(val_x.shape))\n      #\"\\nTest set: \\t\\t{}\".format(test_x.shape))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bdaabe5e87ec5225494c90a4949612163752728b"
      },
      "cell_type": "code",
      "source": "import torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# create Tensor datasets\ntrain_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\nvalid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n#test_data = TensorDataset(torch.from_numpy(test_title))\n\n# dataloaders\nbatch_size = 20\n\n# make sure the SHUFFLE your training data\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\nvalid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n#test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f175b7395ea8d1330f69085275bb47278fc9120c"
      },
      "cell_type": "code",
      "source": "dataiter = iter(valid_loader)\nsample_x, sample_y = dataiter.next()\n\nprint('Sample input size: ', sample_x.size()) # batch_size, seq_length\nprint('Sample input: \\n', sample_x)\n\nprint('Sample label size: ', sample_y.size()) # batch_size\nprint('Sample label: \\n', sample_y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "edf543671c25029c6e2c938e322bafcc34d669c7"
      },
      "cell_type": "code",
      "source": "train_on_gpu=torch.cuda.is_available()\n\nif(train_on_gpu):\n    print('Training on GPU.')\nelse:\n    print('No GPU available, training on CPU.')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "75518a53384c18c026d410c15ec67179a1d49eab"
      },
      "cell_type": "code",
      "source": "import torch.nn as nn\n\nclass Q_RNN(nn.Module):\n    \"\"\"\n    The RNN model that will be used to find insincere questions\n    \"\"\"\n\n    def __init__(self, vocab_size, output_size, embedding_dim,out_channels, hidden_dim, n_layers, drop_prob=0.5):\n        \"\"\"\n        Initialize the model by setting up the layers.\n        \"\"\"\n        super(SentimentRNN, self).__init__()\n\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        # embedding and LSTM layers\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.conv = nn.Conv1d(embedding_dim,hidden_dim,1)\n        self.p1 = nn.AvgPool1d(1)\n        self.lstm = nn.LSTM(hidden_dim, hidden_dim, n_layers, \n                            dropout=drop_prob, batch_first=True)\n        \n        # dropout layer\n        self.dropout = nn.Dropout(0.3)\n        \n        # linear and sigmoid layers\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sig = nn.Sigmoid()\n        \n\n    def forward(self, x, hidden):\n        \"\"\"\n        Perform a forward pass of our model on some input and hidden state.\n        \"\"\"\n        batch_size = x.size(0)\n\n        # embeddings and lstm_out\n        embeds = self.embedding(x)\n        #print(embeds.size())\n        embeds = embeds.transpose(0, 1).transpose(1, 2)\n        #print(embeds.size())\n        embeds = self.conv(embeds)\n        #print(embeds.size())\n        embeds = self.p1(embeds)\n        #print(embeds.size())\n        embeds = embeds.transpose(1, 2).transpose(0, 1)\n        #print(embeds.size())\n        lstm_out, hidden = self.lstm(embeds, hidden)\n    \n        # stack up lstm outputs\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        # dropout and fully-connected layer\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        # sigmoid function\n        sig_out = self.sig(out)\n        \n        # reshape to be batch_size first\n        sig_out = sig_out.view(batch_size, -1)\n        sig_out = sig_out[:, -1] # get last batch of labels\n        \n        # return last sigmoid output and hidden state\n        return sig_out, hidden\n    \n    \n    def init_hidden(self, batch_size):\n        ''' Initializes hidden state '''\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        \n        return hidden",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cd527fa806ec7ea6f1a20d5f838b4c2061623d4d"
      },
      "cell_type": "code",
      "source": "q_size = len(q_vocab_to_int)+1 # +1 for the 0 padding + our word tokens\noutput_size = 1\nembedding_dim = 10\nconv_size = 20\nhidden_dim = 64\nn_layers = 2\n\nnet = SentimentRNN(q_size, output_size, embedding_dim,conv_size, hidden_dim, n_layers)\n\nprint(net)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1b85b919868318e691fb0de1e060ed437e959f7d"
      },
      "cell_type": "code",
      "source": "lr=0.001\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6de88615e74ccac8cca092d7ab15130c0b6e023e",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "epochs = 3 # 3-4 is approx where I noticed the validation loss stop decreasing\n\ncounter = 0\nprint_every = 100\nclip=5 # gradient clipping\n\n# move model to GPU, if available\nif(train_on_gpu):\n    net.cuda()\n\nnet.train()\n# train for some number of epochs\nfor e in range(epochs):\n    # initialize hidden state\n    h = net.init_hidden(batch_size)\n\n    # batch loop\n    for inputs, labels in train_loader:\n        counter += 1\n\n        if(train_on_gpu):\n            inputs, labels = inputs.cuda(), labels.cuda()\n\n        # Creating new variables for the hidden state, otherwise\n        # we'd backprop through the entire training history\n        h = tuple([each.data for each in h])\n\n        # zero accumulated gradients\n        net.zero_grad()\n\n        # get the output from the model\n        output, h = net(inputs, h)\n\n        # calculate the loss and perform backprop\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        nn.utils.clip_grad_norm_(net.parameters(), clip)\n        optimizer.step()\n\n        # loss stats\n        if counter % print_every == 0:\n            # Get validation loss\n            #val_h = net.init_hidden(batch_size)\n            #val_losses = []\n            #net.eval()\n            #for inputs, labels in valid_loader:\n\n                # Creating new variables for the hidden state, otherwise\n                # we'd backprop through the entire training history\n                #val_h = tuple([each.data for each in val_h])\n\n                #if(train_on_gpu):\n                #    inputs, labels = inputs.cuda(), labels.cuda()\n\n               # output, val_h = net(inputs, val_h)\n              #  val_loss = criterion(output.squeeze(), labels.float())\n\n             #   val_losses.append(val_loss.item())\n\n            #net.train()\n            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n                  \"Step: {}...\".format(counter),\n                  \"Loss: {:.6f}...\".format(loss.item()))\n                  #\"Val Loss: {:.6f}\".format(np.mean(val_losses)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4d8b54d2bfe56169b761a5e8abc673790c66a0f5"
      },
      "cell_type": "code",
      "source": "test_data_path = '../input/test.csv'\n\n# read test data file using pandas\ntest_df = pd.read_csv(test_data_path)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0ca0fda40369e4611f87c3e9e8e7c7f4136c5995"
      },
      "cell_type": "code",
      "source": "sample = pd.read_csv('../input/sample_submission.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a7b3158fefc2c4247fcad2c87c69ab167dbaaf04"
      },
      "cell_type": "code",
      "source": "sample.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6b03987ef4c6006458011a42365a15686ed45779"
      },
      "cell_type": "code",
      "source": "test_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "42ba3852306803119684bfee04308220ab62c334"
      },
      "cell_type": "code",
      "source": "test_text = test_df['question_text'].str.replace('[^\\w\\s]','')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "47cbb7fb2b640c15a5288946c0f43269116128dd"
      },
      "cell_type": "code",
      "source": "test_text = test_text.str.split(' ')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "49f5c9ae0a87143667288c87d1b28308fbc892f3"
      },
      "cell_type": "code",
      "source": "\ntest_int=[]\nfor tag in test_text:\n  if isinstance(tag,list): \n      if len(tag)!=0:\n        try:\n          test_int.append([q_vocab_to_int[word] for word in tag])\n        except KeyError as error:\n          test_int.append([0]) \nfor i in range(10):\n    test_int.append([0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f55123666c064da822999cd62da9e7cdb43bdccd",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "seq_length = 25\n\ntest_features = pad_features(test_int, seq_length=seq_length)\n\n## test statements - do not change - ##\nassert len(test_features)==len(test_int), \"Your features should have as many rows as reviews.\"\nassert len(test_features[0])==seq_length, \"Each feature row should contain seq_length values.\"\nprint(test_features[:30,:25])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d06ecf2d605e130972627e2739a5ad7e658c704f"
      },
      "cell_type": "code",
      "source": "len(test_features)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8dcbb56c9999d2ea67f6380865625df553004163"
      },
      "cell_type": "code",
      "source": "\n#np.append(test_features,m)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "81b088f6791e072b948ebe35537209997898660e"
      },
      "cell_type": "code",
      "source": "test_data = TensorDataset(torch.from_numpy(test_features))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bbced5e440b9c3fe17be58f2a9b49825b4d68e2e"
      },
      "cell_type": "code",
      "source": "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "96a8a1117536a44f4b66a12af8e9654d3eb34437"
      },
      "cell_type": "code",
      "source": "op = []\nfor inputs in test_loader:\n    inputs = inputs[0]\n    inputs = inputs.cuda()\n    x,h = net(inputs,h)\n    for i in x:\n        if i<0.5:\n            op.append(0)\n        else:\n            op.append(1)\n            ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c9faab258362ff97ffbea3cbe8ebffbc0ef8f26a"
      },
      "cell_type": "code",
      "source": "op",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7aad6ed812156415ff024d2474bd6774f3f24c9e"
      },
      "cell_type": "code",
      "source": "qid = test_df['qid']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "83a8470fbdcdd4f12e56876ebab5e13ce69d6c54"
      },
      "cell_type": "code",
      "source": "output = pd.DataFrame({'qid': qid,\n                       'prediction': op[:-10]})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b500914796123fab4e78c8c744d3e1261a40340e"
      },
      "cell_type": "code",
      "source": "output.to_csv('submission.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0a2d36f9a3b3ed34d7336fed46ee784872824164"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}